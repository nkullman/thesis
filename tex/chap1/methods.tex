\section{Methods}
\subsection{Simultaneous Provision of Ecosystem Services}
\label{subsec:whyUsingMultiObjModel}
Aiming to determine how climate change may destabilize the relationships between managed ecosystem services, I followed the IPCC's approach of using scenario-based analyses. I selected three climate projections for consideration. The first scenario is the assumption of no climate change. This is the default assumption for many current studies such as \cite{svetlanaDissertation2013}, from which this study is derived. I used this climate scenario as the control against which I  compared the ecosystem service tradeoffs observed in the other scenarios.

The second and third scenarios are ensembles of climate models produced by research agencies recognized by the IPCC and assembled by the USFS \cite{dixon2002essential}. Details about the global circulation models (GCMs) included in the ensembles can be found in \cite{ClimateModelsInFVSEnsemble}. The second scenario is an ensemble of models for Representative Concentration Pathway (RCP) 4.5 $W/m^2$, and the third scenario is the same ensemble of models for RCP 8.5 $W/m^2$. The RCPs indicate the additional radiative forcing (in $W/m^2$) above pre-industrial levels, with higher values of forcing indicative of more severe climate change. I chose these three scenarios because they represent a range of severity of climate change, from a $0 \degree C$ warming by the year 2100 under the control (first scenario) to a $2.6-4.8 \degree C$ warming under the third \cite{ipcc2013climate}.

In order to provide a basis upon which to compare the tradeoffs among ecosystem services, I parameterized and solved a multi-objective mixed integer-linear mathematical program (MIP) for each climate scenario. The benefit of using this approach is that it produces a set of solutions indicating optimal simultaneous achievement in each of the considered ecosystem services. This set of solutions is known as an efficient frontier or a Pareto frontier. The solutions comprising the frontier are optimal, which means that - for ecosystem services which are in competition - an improvement in one ecosystem service cannot be achieved without sacrificing some amount of another. Prior to beginning this work, I determined that the objectives of reducing fire hazard, providing NSO habitat, and reducing sediment delivery were in conflict. Hence, the frontier generated by the multi-objective MIP provides a means to quantitatively study the conflict between these ecosystem services.

\subsubsection{The Multi-objective MIP}
\label{sec:multiObjModel}
The first objective is to minimize the average fuel model at the end of the 80-year planning horizon:
\begin{align}
Minimize \quad & \sum_{i\in I} \sum_{r\in R} F_{i,r} x_{i,r} \label{eqn:objFire}
\end{align}
In equation \eqref{eqn:objFire}, I sum over all stands $i \in I$ and all treatment prescriptions $r \in R$ to obtain a cumulative fire hazard metric. The coefficients $F_{i,r}$ are the area-weighted fuel models of each stand $i \in I$ at the end of the planning horizon if stand $i$ is assigned to treatment prescription $r \in R$. The possible treatment prescriptions $r \in R$ are treat in the first period ($r=1$), treat in the second period ($r=2$), treat in both periods ($r=3$), or do not treat ($r=0$). I determined the type of treatment to be performed for a given stand in a given time period \textit{a priori} depending on silvicultural characteristics, so the only decisions in the model are whether to treat a stand $i$ according to treatment schedule $r$.

To determine the coefficients $F_{i,r}$, the fuel model for each stand under each treatment prescription, I began with the GNN structure map for map year 2012 (\url{http://lemma.forestry.oregonstate.edu/data/structure-maps}) from Oregon State University's Landscape Ecology, Modeling, Mapping \& Analysis (LEMMA) group. I used this data with Climate FVS to project vegetation growth through the end of the 80 year planning horizon, using FVS's Fire and Fuels Extension (FFE) to obtain the average fuel model for each stand. I used a stand's area-weighted fuel model as a proxy for fire hazard, because the higher the fuel model, generally, the larger the fuel loading.

The second objective is to minimize the peak short-term sediment delivery that results from performing treatments in either period one ($S_1$) or period two ($S_2$):
\begin{align}
Minimize \quad \max \{S_1,S_2\} \label{eqn:objSediment}
\end{align}

The last objective is to maximize the area of suitable northern spotted owl habitat at the end of the planning horizon.
\begin{align}
Maximize \quad &\sum_{i\in I_\omega} \left(a_i p_i + e a_i \left( \sum_{j \in R_i} x_{i,j}-p_i \right) \right) \label{eqn:objOwl}
\end{align}
The set of stands $i \in I_\omega$ are those that qualify as NSO habitat under at least one treatment prescription $j \in R_i$. If a stand $i$ does not qualify as NSO habitat under any treatment prescriptions (if the set $R_i = \emptyset$), then $i \notin I_\omega$. If the model assigns a stand $i \in I_\omega$ a treatment prescription $j \in R_i$, then stand $i$ qualifies as NSO habitat at the end of the planning horizon and it contributes to the objective function through a combination of its decision variable $x_{i,j}$ and its cluster inclusion trigger variable $p_i$. If the stand is part of a cluster of stands over 200 ha that all qualify as NSO habitat, then the variable $p_i = 1$, and it contributes an amount equal to the stand's area $a_i$. If stand $i$ qualifies as NSO habitat, but it is not part of a 200 ha cluster of suitable habitat, then it contributes an amount equal to its area $a_i$ discounted by a factor $e$. I define $e = 0.5$ in this model because of the sensitivity and large area requirements of the northern spotted owl \cite{us2011revised}.

A stand's qualifying for northern spotted owl habitat is dependent on three conditions: the presence of trees with DBH no less than 76 cm, average stand elevation less than 1830 m, and canopy cover of at least 60\%. I used the same vegetation data to determine stands' NSO habitat suitability as that used to determine the fuel model coefficients $F_{i,r}$.

The objectives are subject to the following constraints. First, I define accounting variables for the sediment delivery that results from the performance of the prescribed management actions.
\begin{align}
\sum_{i\in I} \sum_{r\in 1,3} s_{i,r} x_{i,r} = S_1 \label{eqn:constraintSediment1}\\
\sum_{i\in I} \sum_{r\in 2,3} s_{i,r} x_{i,r} = S_2 \label{eqn:constraintSediment2}
\end{align}
The coefficients $s_{i,r}$ are the amount of sediment (in tonnes) that would result from treating stand $i$ according to prescription schedule $r$. To determine the sediment coefficients, I employed the online GIS-based Watershed Erosion Prediction Project (WEPP) tool \cite{frankenberger2011development}. To account for climate change, I used the climate data generated by Climate FVS to create custom climate files for the WEPP simulations.

In order to control the trigger variables $p_i$ indicating a stand's inclusion in a 200 ha cluster of NSO habitat, I used the following two constraints:
\begin{align}
\sum_{i \in D_c} \sum_{j \in R_i} x_{i,j} - |c| q_c &\ge 0 \qquad \forall c \in C \label{eqn:constraintClusterTriggers}\\
\sum_{c \in C_i} q_c - p_i &\ge 0 \qquad \forall i \in I_\omega \label{eqn:constraintPVarTriggers}
\end{align}
$C$ is the set of all clusters whose combined area is greater than 200 ha. Equation \eqref{eqn:constraintClusterTriggers} specifies that all stands $i \in D_c$ within a cluster $c \in C$ must be assigned a management prescription such that they meet all NSO habitat criteria in order for the cluster trigger variable $q_c$ to take value 1.

Equation \eqref{eqn:constraintPVarTriggers} specifies that if no cluster $c \in C_i$ - the set of clusters that contain site $i$ - meets NSO qualifications, then the trigger variable $p_i$ must equal 0. If some cluster $c \in C_i$ does meet NSO qualifications, then the objective function \eqref{eqn:objOwl} will draw up the value of variable $p_i$ to 1.

I also enforce the restriction that each stand may be assigned to at most one treatment prescription.
\begin{align}
\sum_{r \in R} x_{i,r} = 1  \qquad \forall i \in I \label{eqn:constraintOnePrescrip}
\end{align}

Next, I ensured that the area treated in each time period is less than a prespecified maximum area $A$:
\begin{align}
\sum_{i \in I} \sum_{r \in 1,3} a_i x_{i,r} &= H_1 \label{eqn:constraintAreaAcctg1}\\
\sum_{i \in I} \sum_{r \in 2,3} a_i x_{i,r} &= H_2 \label{eqn:constraintAreaAcctg2}\\
H_1 &\le A \label{eqn:constraintAreaRestr1}\\
H_2 &\le A \label{eqn:constraintAreaRestr2}
\end{align}
where the first two equations define the accounting variables for the areas treated in time periods 1 and 2, $H_1$ and $H_2$.

Finally, I specified fluctuation constraints to bound the differences in area treated in between time periods:
\begin{align}
\ell H_1 - H_2 &\le 0 \label{eqn:constraintAreaFlucL}\\
-u H_1 + H_2 &\le 0 \label{eqn:constraintAreaFlucU}
\end{align}
I define a maximum of 20\% fluctuation between time periods. That is, $\ell = 0.8$ and $u = 1.2$.

Together with the binary specifications on our variables in equation \eqref{eqn:constraintNonNeg}, the complete model is
\begin{align*}
Minimize \quad & \\
&\sum_{i\in I,r\in R} F_{i,r} x_{i,r}\\
&\max \{S_1,S_2\}\\
Maximize \quad & \\
&\sum_{i\in I_\omega} \left(a_i p_i + e a_i \left( \sum_{j \in R_i} x_{i,j}-p_i \right) \right)
\end{align*}

Subject to:
\begin{align}
\sum_{i\in I} \sum_{r\in 1,3} s_{i,r} x_{i,r} &= S_1 \notag\\
\sum_{i\in I} \sum_{r\in 2,3} s_{i,r} x_{i,r} &= S_2 \notag\\
\sum_{i \in D_c} \sum_{j \in R_i}x_{i,j} - |c| q_c &\ge 0 \qquad \forall c \in C \notag\\
\sum_{c \in C_i} q_c - p_i &\ge 0 \qquad \forall i \in I_\omega \notag\\
\sum_{r \in R} x_{i,r} &= 1  \qquad \forall i \in I \notag\\
\sum_{i \in I} \sum_{r \in 1,3} a_i x_{i,r} &= H_1 \notag\\
\sum_{i \in I} \sum_{r \in 2,3} a_i x_{i,r} &= H_2 \notag\\
H_1 &\le A \notag\\
H_2 &\le A \notag\\
\ell H_1 - H_2 &\le 0 \notag\\
-u H_1 + H_2 &\le 0 \notag\\
x_{i,r}, p_i, q_c \in \{0,1\} \quad &\forall i \in I, r \in R, c \in C \label{eqn:constraintNonNeg}
\end{align}


\subsection{Comparing Tradeoffs under each Climate Change Scenario}
Solving the model described in Section \ref{subsec:whyUsingMultiObjModel} generates a set of non-dominated points comprising an efficient frontier. As we have a model for each climate scenario, we have an efficient frontier for each scenario. Each point on the frontier achieves a certain amount in each objective with no improvement possible in one objective without sacrificing achievement in one of the others. The structure of a frontier therefore provides information about the tradeoffs and the conflicting relationships between the ecosystem services. Comparing the frontiers then allows us to compare the conflicting relationships among the ecosystem services between climate scenarios.

The best means of comparing efficient frontiers is "neither trivial nor definite" (PyGMO hypervolume indicator page). We draw from ideas from the field of heuristics for solving multiobj opt problems. In those cases, the comparison of frontiers allows the developers of the heuristics to compare their search methods against others (by comparing their resulting frontiers against a controlled non-dominated set). In our case, comparing frontiers has physical meaning regarding management actions and how climate will impact relationships between forest ecosystem services.

\subsubsection{Slopes between points or something}
Like summing the squares of the slopes in between two objectives for a given solution. Think of parallel coordinates plot. How to capture the fact that some objectives have a bunch of straight lines going between them (or lines going every which way) and others have a very clear crisscrossing? I think I have a note to self somewhere on "little-master" PC about this. 

\subsubsection{Computing a Frontier's Solution Spacing}
The spacing of solutions along the frontier provide a measure of flexibility for the decision maker. The more solutions 

\subsubsection{Computing the average distance to the ideal solution}
Computing the average distance from a point on the frontier to the ideal solution is another means of comparing frontiers. The shorter the average distance from the frontier to the ideal solution, in general, the less conflict between the objectives. Conversely, a large distance from the frontier to the ideal solution implies strong conflict between the objectives.

\subsubsection{Computing a Frontier's Hypervolume Indicator}
To compare the tradeoff structure of each climate change scenario's corresponding Pareto frontier, I calculated the relative volume of the objective space bound by the frontier.  Computing such a volume for a two-dimensional frontier is trivial. Consider figure \ref{fig:2DFrontierVol}.
\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\textwidth]{"../images/2DFrontierVolumeExample"}
  \caption{A two-dimensional frontier. The volume of this frontier may be computed by summing the areas of the rectangles shown.}
  \label{fig:2DFrontierVol}
\end{figure}
The reader can imagine a process to compute the volume whereby the frontier is divided into rectangles, as shown, and then summing the areas of these rectangles to get the total frontier volume.

Performing a similar computation in three and higher dimensions is less trivial and is an area of active research !CITESOMEONE. The higher-order volume computation is also often accomplished using Monte Carlo simulation !CITE SOMEONE.

I developed the following recursive algorithm to exactly compute the volume of an $n$-dimensional frontier for $n>2$.

Given a set of Pareto optimal solutions $\mathcal{P}$ to a multi-objective mathematical programming model with a set of objectives $O$ of cardinality $N := |O|$, this algorithm computes the volume $V$ of the objective space bounded by the Pareto frontier defined by the solutions $x \in \mathcal{P}$. The objectives are assumed to be normalized so that the objective space is the $N$-dimensional unit hypercube with the origin and the point $\vec{\mathbf{1}}$ defining the nadir objective vector and the ideal objective vector, respectively. That is, all objectives are assumed to be maximized.

We project the objective space into $N-1$ dimensions by eliminating the dimension associated with an (arbitrarily-chosen) objective $p \in O$. We define the set of objectives $\overbar{O} := O \backslash \{p\}$. It is assumed that $x \in \mathcal{P}$ are sorted in descending order according to $p$. The algorithm proceeds by sequentially adding solutions to the ($N-1$)-dimensional space, and calculating the contribution to the frontier volume as a product of the volume contribution in $N-1$ dimensions and its achievement in objective $p$.

Let
$\overbar{V_x}$ be the ($N-1$)-dimensional volume contribution of solution $x$ and
$x_p$ be the achievement of solution $x$ in objective $p$. Further, let
$F$ be the set of non-dominated solutions in $N-1$ dimensions.
We proceed to compute the $N$-dimensional volume of the frontier $V$ as follows.

\begin{figure}[!ht]
\caption{Algorithm to compute the volume of a Pareto frontier}
\begin{algorithmic}[1]

\State $V \gets 0$
\State $\overbar{V} \gets 0$
\State $F \gets \emptyset$

% Iterate over each solution
\ForAll{$x \in \mathcal{P}$}

	\State $\overbar{V}_x \gets \prod_{o \in \overbar{O}} x_{o} - \overbar{V}$
		
	\ForAll{$f \in F$}
		\If{$f_o < x_o \forall o \in \overbar{O}$}
			\State $F \gets F \backslash \{f\}$
		\EndIf
	\EndFor
	
	% iterate over subdimensions to "add back the sides"	
	\ForAll{$o \in \overbar{O}$}
	
		\State $F_{x,o} := \set{f \in F : f_o > x_o}$
		
		\State Sort $f \in F_{x,o}$ in ascending order by their $o$th component, $f_o$
		
		\State $v_i \gets x_o$
		\ForAll{$f \in F_{x,o}$}
			\State $v_t \gets f_o$
			\State $\delta_o :	= v_t - v_i$
			\State $\overbar{V}_x \gets \overbar{V}_x + \delta_o \prod_{\sigma \in \overbar{O} \backslash \{o\}} f_\sigma$
			\State $v_i \gets v_t$
		\EndFor
		
	\EndFor
	
	\State $F \gets F \cup \{x\}$
	\State $\overbar{V} \gets \overbar{V} + \overbar{V}_x$
	\State $V \gets V + x_p \overbar{V}_x$
\EndFor


\end{algorithmic}
\end{figure}

The result of the algorithm is a single metric for each frontier, known as the hypervolume indicator !CITESOMEONE. This metric is used in the field of Evolutionary Algorithms for MultiObjOpt. 