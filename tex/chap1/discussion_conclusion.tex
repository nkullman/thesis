\section{Discussion \& Conclusion}
% We did good
We used a case study of the impact of climate change on the joint provision of forest ecosystem services to successfully demonstrate the utility of a new measure of pairwise objective conflict and to demonstrate a new application of existing conflict measures in the quantification of conflict within and among multi-objective systems.

% We did good in something that was hard to do good in
The case study served as a rigorous first test of the process and the conflict measures, because there was little overall conflict in these systems, and the differences across climate scenarios were not great in terms of relative objective achievement. For instance, in the case study, the hypervolume for each climate scenarios was relatively large with solutions occupying over 80\% of the objective space in all cases. In addition, in all but one pairwise objective comparison, it was difficult to discern any distinct conflict relationship between the objectives. As a result, our proposed conflict metric and the hypervolume indicators were required to detect subtle difference in conflict, which they did successfully. Should the difference in objective achievement between climate scenarios have been more pronounced, or should the objectives have been in greater conflict with one another, we suspect the utility of these measures and the process we demonstrated here would only increase.

% But who's to say that it will always do good. It may not, but to check we need the tool, and that's what we came up with.
We saw that it worked for us, but the results may be different if under diff circumstances. But we need to be able to perform this sort of analysis, and that is why we came up with what we did. We showed that it was useful here.

% You might also look at this thing to see if it does good here
For instance, if a food processing facility HAD BLANK SOME LARGE SWEEPING MACHINE-TECH CHANGE, then BLANK WOULD HAPPEN AND OUR CONFLICT MEASURE WOULD BE GREAT TO DETECT THESE DIFFS.

% Or this other thing
Or if some regulatory change happened to hospitals LIKE BLANK, then YOU MIGHT NOTICE THESE ENORMOUS DIFFS AND AGAIN OUR THING WOULD BE GREAT.

% We think our thing would continue to do good, bc, again, it did good here. It was so great.
Based off of our results, we believe that our new measure and the process which we suggest here would be useful in these cases. The EMO measures detected increasing system-level conflict, which our sed/owl/fire data backed up. We also had the new conflict measure report more conflict for fire-sed than it did for the others, which is good (bc that's what we expected).

% But it's not perfect, you know?
However, the tool is not without its shortcomings. We first note that differences in low-level-of-conflict scenarios should be investigated more thoroughly, bc Cij may vary widely when both its components are small. In these cases, the individual components have more influence, and so slight variations in them can lead to large diffs in Cij. Also, when looking at the combination of all Cij measures for objs within a frontier, their combination does not give you details about how the hypervol measure may vary. For instance, the lowest hypervol scenario for us also had the lowest sum of Cijs.

% So we need to do more research, more case studies, and we need to try to make our tool better.
In sum, more case studies should be done using this conflict measure and the others old ones we used here. Additionally, refinements to the conflict measure should be investigated, especially in the case where ran correlation and avg distance to ideal are both mid-range.

% But suffice it to say, we're awesome. Over and out.	
But all we've done is great, really. The end.